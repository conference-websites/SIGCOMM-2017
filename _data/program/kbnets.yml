---
- type: day
  time: Monday, August 21, 2017
  room: Laureate Room (Luskin Center)
  title: ''
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 9:00am - 9:15am
  room: Laureate Room (Luskin Center)
  title: Opening Remarks
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 9:15am - 10:20am
  room: Laureate Room (Luskin Center)
  title: Keynote 1
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: keynote
  time: ''
  room: ''
  title: 'Towards A Software Defined Data Plane for Datacenters

'
  authors: Arvind Krishnamurthy
  abstract: |-
    <p>Emerging networking architectures are allowing for flexible and reconfigurable packet processing at line rate both at the NIC and the switch.  These emerging technologies address a key limitation with software defined networking solutions such as OpenFlow, which allow for custom handling of flows only as part of the control plane.  Many network protocols, such as those that perform resource allocation, require per-packet processing, which is feasible only if the data plane can be customized to the needs of the protocol.  These new technologies thus have the potential to address this limitation and truly enable a "Software Defined Data Plane" that provides greater performance and isolation for datacenter applications.</p>
    <p>Despite their promising new functionality, intelligent NICs and flexible switches are not all-powerful; they have limited state, support limited types of operations, and limit per-packet computation in order to be able to operate at line rate.  Recent work addresses some of these limitations by providing a set of general building blocks that mask these limitations using approximation techniques and thereby enabling the implementation of realistic network protocols and distributed systems.  But this represents just a first step towards developing an understanding as to how to enable a Software Defined Data Plane and how to best leverage that for applications.  This talk will survey both recent work and discuss future directions to fully realize the potential of recent and upcoming hardware.</p>
  bio: Arvind Krishnamurthy is a Professor of Computer Science and Engineering at
    the University of Washington. His research interests span all aspects of building
    practical and robust computer systems and is aimed at making improvements to the
    robustness, security, and performance of Internet-scale systems. A recent focus
    of his work has been to develop ways to dramatically improve the performance of
    networked applications deployed inside datacenters by rearchitecting all layers
    of the datacenter software stack.
  photo: images/speakers/Arvind-Krishnamurthy.jpg
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: break
  time: 10:20am - 10:50am
  room: Foyer
  title: Coffee Break
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 10:50am - 12:30pm
  room: Laureate Room (Luskin Center)
  title: 'Session 1: High Performance Networks & Apps'
  authors: Costin Raiciu (University Politehnica of Bucharest)
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: LogMemcached - An RDMA based Continuous Cache Replication
  authors: Samyon Ristov (The Hebrew University of Jerusalem, Israel), Yaron Weinsberg
    (Microsoft), Danny Dolev (The Hebrew University of Jerusalem, Israel), and Tal
    Anker (Mellanox Technologies)
  abstract: |
    <p>One of the advantages of cloud computing is its ability to quickly scale out services to meet demand. A common technique to mitigate the increasing load in these services is to deploy a cache.</p>
    <p>Although it seems natural that the caching layer would also deal with availability and fault tolerance, these issues are nevertheless often ignored, as the cache has only recently begun to be considered a critical system component. A cache may evict items at any moment, and so a failing cache node can simply be treated as if the set of items stored on that node have already been evicted. However, setting up a cache instance is a time-consuming operation that could inadvertently affect the service’s operation.</p>
    <p>This paper addresses this limitation by introducing cache replication at the server side by expanding Memcached (which currently provides availability via client side replication). This paper presents the design and implementation of LogMemcached, a modification of Memcached’s internal data structures to include state replication via RDMA to provide an increased system availability, improved failure resilience and enhanced load balancing capabilities without compromising performance and with introducing a very low CPU load, while keeping the main principles of Memcached’s Design Philosophy.</p>
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34781"
  slides: ''
  video: "//dl.acm.org/authorize?N34781"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Accelerating Open vSwitch with Integrated GPU
  authors: Janet Tseng, Ren Wang, James Tsai, Yipeng Wang, and Charlie Tai (Intel
    Labs)
  abstract: "<p>With the fast development of Software Defined Networking (SDN) and
    network virtualization, software-based network virtual switches have emerged as
    a critical component to provide network services to VMs. Among virtual switches,
    Open vSwitch (OvS) is an open source virtual switch implementation commonly used
    and well-studied. Using Data Plane Development Kit (DPDK) with OvS to bypass the
    OS kernel and process packets in userspace provides tremendous performance benefits
    on general purpose platforms. Integrated GPUs, residing on the same die with the
    CPU on general purpose platforms, offering many advanced features such as on-chip
    interconnect CPU-GPU communication, and sharing physical/virtual memory, become
    a promising additional compute resource to further accelerate the OvS process.
    In this paper, we design and implement an inline GPU assisted OvS architecture,
    via offloading the expensive tuple space search to GPU and balancing switching
    processing between CPU and GPU. We evaluated the performance on an Intel® Xeon®
    processor of the E3-1575M v5 product family (code-name Skylake) with an integrated
    GT4e GPU. The results show that our proposed architecture improved the OvS throughput
    by 3x, compared to the optimized CPU-only OvS-DPDK implementation.</p>"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34782"
  slides: ''
  video: "//dl.acm.org/authorize?N34782"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: 'VIRTIO-USER: A New Versatile Channel for Kernel-Bypass Networks'
  authors: Jianfeng Tan, Cunming Liang, Huawei Xie, Qian Xu, Jiayu Hu, Heqing Zhu,
    and Yuanhan Liu (Intel)
  abstract: |
    <p>Kernel-Bypass Networks still faces some challenging problems: (1) it’s hard for container to gain better performance by kernel-bypass virtual switch; (2) it lacks stable and efficient way to inject packets back to kernel stack for kernel-bypass network interface.</p>
    <p>To solve above problems, we propose VIRTIO-USER, as a versatile, performant, secure and standardized channel. Instead of using hypervisor to bridge frontend and backend driver, we implement an embedded vhost adapter in frontend driver to talk with vhost backend directly. It keeps other mechanisms like memory sharing model, ring layout and feature negotiation, in the same way with VIRTIO. We implement and upstream it into DPDK. In comparison with kernel-based container networking and existing exception path solution, our evaluation shows +3.5x performance boost in both scenarios.</p>
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34783"
  slides: ''
  video: "//dl.acm.org/authorize?N34783"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Towards a Scalable Modular QUIC Server
  authors: Yufeng Duan (Politecnico di Torino), Massimo Gallo (Nokia Bell Labs), Stefano
    Traverso (Politecnico di Torino), Rafael Laufer (Nokia Bell Labs), and Paolo Giaccone
    (Politecnico di Torino)
  abstract: "<p>QUIC has been recently proposed as an alternative transport protocol
    for web services requiring both low latency and end-to-end encryption. In a different
    direction, recent kernel-bypass techniques enabling high-speed packet I/O have
    fostered the development of scalable middleboxes and servers with the introduction
    of user-space network stacks. Attempting to join the best of both solutions, we
    introduce in this paper a modular L2–L7 network stack in user space based on QUIC.
    Our modular and scalable QUIC transport protocol called cQUIC is implemented in
    Click and uses Intel<sup></sup> DPDK for high-speed packet I/O. We prototype cQUIC
    and show at least an order of magnitude improvement over the Google QUIC server.
    We also show that cQUIC scalability is CPU (and not I/O) bounded due to the high
    cost of cryptographic operations. From real-world traffic traces, we observe that
    up to 18% of QUIC connections are established using the expensive 2-RTT handshake,
    limiting scalability further.</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34784"
  slides: ''
  video: "//dl.acm.org/authorize?N34784"
  COL_UID: ''
- type: break
  time: 12:30pm - 1:45pm
  room: Centennial Terrace
  title: Lunch Break
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 1:45pm - 2:50pm
  room: Laureate Room (Luskin Center)
  title: Keynote 2
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: keynote
  time: ''
  room: ''
  title: Building Hardware-Accelerated Networks at Scale in the Cloud
  authors: Daniel Firestone
  abstract: |-
    <p>All modern clouds are built on top of software-defined networking for flexibility, programmability, and scale-out. In the last several years, as server densities have risen along with NIC speeds to 40/50GbE and beyond, numerous hardware acceleration techniques have been popularized claiming to solve software networking’s scalability problems at high speeds – SR-IOV is said to remove the overhead of virtualization, DPDK should bring the cost of packet processing on cores to near-zero, and RDMA eliminates the TCP stack and replaces it with a low-latency hardware transport layer. But what does it take to really deploy and use these technologies widely in a public cloud? Default implementations often lack the programmability, flexibility, serviceability, visibility, diagnosability, and reliability that we'd need at scale.</p>
    <p>In Microsoft Azure, we’ve been working to operationalize these offloads for years and have seen their ups and downs. In this talk we'll review our experiences building and deploying technologies like Azure Accelerated Networking, and RDMA-based scale-out storage, into our high speed software-defined network. We’ll discuss changes to offloads we needed and technologies we had to build on top, such as the Azure SmartNIC, to enable reliability and flexibility at scale. Finally, we’ll discuss some principles that we think are fundamental to building any successful hardware-accelerated network at scale such as ours, and open problems and challenges we see going forward.</p>
  bio: Daniel Firestone is the Tech Lead and Manager for the Azure Host Networking
    Group at Microsoft. His team builds the Azure virtual switch, which serves as
    the datapath for Azure virtual networks, as well as SmartNIC, the Azure platform
    for offloading host network functions to reconfigurable FPGA hardware, and Azure’s
    RDMA stack. Before Azure, Daniel did his undergraduate studies at MIT.
  photo: images/speakers/Daniel-Firestone.png
  link: ''
  slides: files/program-kbnets/keynote-2.pdf
  video: ''
  COL_UID: ''
- type: session
  time: 2:50pm - 3:40pm
  room: Laureate Room (Luskin Center)
  title: 'Session 2: Congestion Control'
  authors: Hongqiang Liu (Microsoft Research)
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: 'RoCE Rocks without PFC: Detailed Evaluation'
  authors: Alexander Shpiner, Eitan Zahavi, Omar Dahley, Aviv Barnea, Rotem Damsker,
    Gennady Yekelis, Michael Zus, Eitan Kuta, and Dean Baram (Mellanox Technologies)
  abstract: |
    <p>In recent years, the usage of RDMA in data center networks has increased significantly, with RDMA over Converged Ethernet (RoCE) emerging as the canonical approach to deploying RDMA in Ethernet-based data centers. Initial implementations of RoCE required a lossless fabric for optimal performance. This is typically achieved by enabling Priority Flow Control (PFC) on Ethernet NICs and switches. The RoCEv2 specification introduced RoCE congestion control, which allows throttling the transmission rate in response to congestion. Consequently, packet loss is minimized and performance is maintained, even if the underlying Ethernet network is lossy.</p>
    <p>In this paper, we discuss the latest developments in RoCE congestion control. Hardware congestion control reduces the latency of the congestion control loop; it reacts promptly in the face of congestion by throttling the transmission rate quickly and accurately. The short control loop also prevents network buffers from overfilling under various congestion scenarios. In addition, fast hardware retransmission complements congestion control in severe congestion scenarios, by significantly reducing the performance penalty of packet drops. We survey architectural features that allow deployment of RoCE over lossy networks and present real lab test results.</p>
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34795"
  slides: ''
  video: "//dl.acm.org/authorize?N34795"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Sharing CPUs via endpoint congestion control
  authors: Laura Vasilescu, Vladimir Olteanu, and Costin Raiciu (University Politehnica
    of Bucharest)
  abstract: |
    <p>Software network processing relies on dedicated cores and hardware isolation to ensure appropriate throughput guarantees. Such isolation comes at the expense of low utilization in the average case, and severely restricts the number of network processing functions one can execute on a host.</p>
    <p>In this paper we propose that multiple processing functions should simply share a CPU core, turning the CPU into a special type of “link”. We use multiple NIC receive queues and the Fastclick suite to test the feasibility of this approach. We find that, as expected, per core throughput decreases when more processes are contending, however the decrease is not dramatic: around 10 Finally, we implement and test in simulation a solution that enables efficient CPU sharing by sending congestion signals proportional to per-packet cost for each flow. This enables endpoint congestion control (e.g. TCP) to react appropriately and share the CPU fairly.</p>
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34796"
  slides: ''
  video: "//dl.acm.org/authorize?N34796"
  COL_UID: ''
- type: break
  time: 3:40pm - 4:10pm
  room: Foyer
  title: Coffee Break
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 4:10pm - 5:25pm
  room: Laureate Room (Luskin Center)
  title: 'Session 3: Measurement & Performance Analysis'
  authors: Eitan Zahavi (Mellanox)
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: How to Measure the Killer Microsecond
  authors: Mia Primorac, Edouard Bugnion, and Katerina Argyraki (EPFL)
  abstract: |
    <p>Datacenter-networking research requires tools to both generate traffic and accurately measure latency and throughput. While hardware-based tools have long existed commercially, they are primarily used to validate ASICs and lack flexibility, e.g. to study new protocols. They are also too expensive for academics. The recent development of kernel-bypass networking and advanced NIC features such as hardware timestamping have created new opportunities for accurate latency measurements. This paper compares these two approaches, and in particular whether commodity servers and NICs, when properly configured, can measure the latency distributions as precisely as specialized hardware.</p>
    <p>Our work shows that well-designed commodity solutions can capture subtle differences in the tail latency of stateless UDP traffic. We use hardware devices as the ground truth, both to measure latency and to forward traffic. We compare the ground truth with observations that combine five latency-measuring clients and five different port forwarding solutions and configurations. State-of-the-art software such as MoonGen that uses NIC hardware timestamping provides sufficient visibility into tail latencies to study the effect of subtle operating system configuration changes. We also observe that the kernel-bypass-based T-Rex software, that only relies on the CPU to timestamp traffic, can also provide solid results when NIC timestamps are not available for a particular protocol or device.</p>
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34797"
  slides: ''
  video: "//dl.acm.org/authorize?N34797"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Performance Isolation Anomalies in RDMA
  authors: Yiwen Zhang, Juncheng Gu, Youngmoon Lee, Mosharaf Chowdhury, and Kang G.
    Shin (University of Michigan)
  abstract: |
    <p>To meet the increasing throughput and latency demands of modern applications, many operators are rapidly deploying RDMA in their datacenters. At the same time, developers are re-designing their software to take advantage of RDMA’s benefits for individual applications. However, when it comes to RDMA’s performance, many simple questions remain open.</p>
    <p>In this paper, we consider the performance isolation characteristics of RDMA. Specifically, we conduct three sets of experiments – three combinations of one throughput-sensitive flow and one latency-sensitive flow – in a controlled environment, observe large discrepancies in RDMA performance with and without the presence of a competing flow, and describe our progress in identifying plausible root-causes.</p>
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34798"
  slides: ''
  video: "//dl.acm.org/authorize?N34798"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Design Challenges for High Performance, Scalable NFV Interconnects
  authors: Guyue Liu (The George Washington University), K.K. Ramakrishnan (University
    of California, Riverside), Mike Schlansker and Jean Tourrilhes (Hewlett Packard
    Labs), and Timothy Wood (The George Washington University)
  abstract: "<p>Software-based network functions (NFs) have seen growing interest.
    Increasingly complex functionality is achieved by having multiple functions chained
    together to support the required network-resident services. Network Function Virtualization
    (NFV) platforms need to scale and achieve high performance, potentially utilizing
    multiple hosts in a cluster. Efficient data movement is crucial, a cornerstone
    of kernel bypass. Moving packet data involves delivering the packet from the network
    interface to an NF, moving it across functions on the same host, and finally across
    yet another network to NFs running on other hosts in a cluster/data center. In
    this paper we measure the performance characteristics of different approaches
    for moving data at each of these levels. We also introduce a new high performance
    inter-host interconnect using InfiniBand. We evaluate the performance of Open
    vSwitch and the OpenNetVM NFV platform, considering a simple forwarding function
    and Snort, a popular intrusion detection system.</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34799"
  slides: ''
  video: "//dl.acm.org/authorize?N34799"
  COL_UID: ''
