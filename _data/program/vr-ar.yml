---
- type: day
  time: Friday, August 25, 2017
  room: Illumination Room (Luskin Center)
  title: ''
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 9:00am - 9:15am
  room: Illumination Room (Luskin Center)
  title: Opening Remarks
  authors: Yong Liu
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: keynote
  time: 9:15am - 10:30am
  room: Illumination Room (Luskin Center)
  title: 'Keynote Talk: AR/VR and the Future of Networked Intelligence'
  authors: 'Dr. Tish Shute, Director, AR/VR, Corporate Strategy and Technology Planning,
    Huawei, USA. '
  abstract: This talk will look at how, given a major shift in computing from “Mobile
    First” to “AI First,” we need to re-examine our current understandings of AR/VR.  I
    will explore how AR/VR and experiential communications will redefine the relationship
    between human and machine intelligence.  This new, intimate connection between
    AR/VR and AI will be key to identifying future network architectures as we begin
    an era in which the augmentation of humans will not be so much about upgrading
    our external tools as merging with them.
  bio: Tish Shute is Director, AR/VR, Corporate Strategy and Technology Planning,
    Huawei, USA. Previously Tish worked with Will Wright (Sims, Sim City) to create
    a new generation of mobile social experiences based on neural nets and innovative
    approaches to machine learning. At Stupid Fun Club Tish worked with Will on next
    generation entertainment - smart toys, augmented reality television and games.
    Tish has taken a leading role in the emergence of augmented and virtual reality
    into the consumer market. She is co-founder of Augmented Reality.ORG, a global
    not-for-profit organization dedicated to advancing augmented and virtual reality
    (AR and VR). She also co-founded Augmented Reality Event (ARE) and Augmented World
    Expo (AWE) which is now in its eighth year and the world’s largest event focused
    on AR and VR. Tish is a recognized speaker in the AR and VR industry and sought-after
    advisor for augmented and virtual reality initiatives. Tish began her career in
    design and technology doing visual effects for film, television, theme parks and
    aerospace. Tish’s first company NPlus1 pioneered the use of automation and robotics
    in film making and entertainment experiences. Tish has an MFA in Combined Media,
    Hunter College, NY, and an MPhil. (Ph.D. ABD) in Culture and Media from New York
    University.
  photo: images/speakers/Tish-Shute.jpg
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: break
  time: 10:30am - 11:00am
  room: Foyer
  title: Coffee Break
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 11:00am - 12:30pm
  room: Illumination Room (Luskin Center)
  title: Session 1
  authors: 'Simone Mangiante '
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Characterization of 360-degree Videos
  authors: Shahryar Afzal, Jiasi Chen, and K. K. Ramakrishnan (University of California,
    Riverside)
  abstract: "<p>Online streaming of Virtual Reality and 360-degree videos is rapidly
    growing, as more and more major content providers and news outlets adopt the format
    to enrich the user experience. We characterize 360-degree videos by examining
    several thousand YouTube videos across more than a dozen categories. 360-degree
    videos, at first sight, seem to pose a challenge for the network to stream because
    of their substantially higher bit rates and larger number of resolutions. However,
    a careful examination of video characteristics reveals that there are significant
    cant opportunities for reducing the actual bit rate delivered to client devices
    based on the user’s field of view. We study the bit rate and the motion in 360-degree
    videos, and compare them against regular videos by investigating several important
    metrics. We find that 360-degree videos are less variable in terms of bit rate,
    and have less motion than regular videos. Our expectation is that variability
    in the bit rates due to the motion of the camera in regular videos (or switching
    between cameras) is now translated to responsiveness requirements for end-to-end
    360-degree streaming architectures.</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34701"
  slides: ''
  video: "//dl.acm.org/authorize?N34701"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: VR Video Conferencing over Named Data Networks
  authors: Liyang Zhang, Syed Obaid Amin, and Cedric Westphal (Huawei Research Center)
  abstract: "<p>We propose a VR video conferencing system over named data networks
    (NDN). The system is designed to support real-time, multi-party streaming and
    playback of 360 degree video on a web player. A centralized architecture is used,
    with a signaling server to coordinate multiple participants. To ensure real-time
    requirement, a protocol featuring prefetching is used for producer-consumer communication.
    Along with the native support of multicast in NDN, this design is expected to
    better support large amount of data streaming between multiple users. As a proof
    of concept, a protoype of the system is implemented with one-way real-time 360
    video streaming. Experiments show that seamless streaming and interactive playback
    of 360 video can be achieved with low latency. Therefore, the proposed system
    has the potential to provide immersive VR experience for real-time multi-party
    video conferencing.</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34702"
  slides: ''
  video: "//dl.acm.org/authorize?N34702"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Prioritized Buffer Control in Two-tier 360 Video Streaming
  authors: Fanyi Duanmu, Eymen Kurdoglu, Amir Hosseini, Yong Liu, and Yao Wang (New
    York University)
  abstract: "<p>360 degree video compression and streaming is one of the key components
    of Virtual Reality (VR) applications. In 360 video streaming, a user may freely
    navigate through the captured 3D environment by changing her desired viewing direction.
    Only a small portion of the entire 360 degree video is watched at any time. Streaming
    the entire 360 degree raw video is therefore unnecessary and bandwidth-consuming.
    One the other hand, only streaming the video in the user’s current view direction
    will introduce streaming discontinuity whenever the user changes her view direction.
    In this work, a two-tier 360 video streaming framework with prioritized buffer
    control is proposed to effectively accommodate the dynamics in both network bandwidth
    and viewing direction. Through simulations driven by real network bandwidth and
    viewing direction traces, we demonstrate that the proposed framework can significantly
    outperform the conventional 360 video streaming solutions.</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34703"
  slides: ''
  video: "//dl.acm.org/authorize?N34703"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Ultra Wide View Based Panoramic VR Streaming
  authors: Ran Ju, Jun He, Fengxin Sun, Jin Li, Feng Li, Jirong Zhu, and Lei Han (Huawei
    Technologies)
  abstract: "<p>Online VR streaming faces great challenges such as the high throughput
    and real time interaction requirement. In this paper, we propose a novel ultra
    wide view based method to stream high quality VR on Internet at low bandwidth
    and little computation cost. First, we only transmit the region where user is
    looking at instead of full 360 degree view to save bandwidth. To achieve this
    goal, we split the source VR into small grid videos in advance. The grid videos
    are able to reconstruct any view flexibly in user end. Second, according to the
    fact that users generally interact at low speed, we expand the view that user
    requested to meet the real time interaction requirement. Besides, a low resolution
    full view stream is supplied to handle exceptional cases such as high speed view
    change. We test our solution in an experimental network. The results show remarkable
    bandwidth saving of over 60</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34704"
  slides: ''
  video: "//dl.acm.org/authorize?N34704"
  COL_UID: ''
- type: break
  time: 12:30pm - 2:00pm
  room: Centennial Terrace
  title: Lunch Break
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 2:00pm - 3:30pm
  room: Illumination Room (Luskin Center)
  title: Session 2
  authors: Jiasi Chen
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: On the Networking Challenges of Mobile Augmented Reality
  authors: Wenxiao ZHANG (Hong Kong University of Science and Technology), Bo HAN
    (AT&T Labs -- Research), and Pan HUI (Hong Kong University of Science and Technology)
  abstract: "<p>In this paper, we conduct a reality check for Augmented Reality (AR)
    on mobile devices. We dissect and measure the cloud-offloading feature for computation-intensive
    visual tasks of two popular commercial AR systems. Our key finding is that their
    cloud-based recognition is still not mature and not optimized for latency, data
    usage and energy consumption. In order to identify the opportunities for further
    improving the Quality of Experience (QoE) for mobile AR, we break down the end-to-end
    latency of the pipeline for typical cloud-based mobile AR and pinpoint the dominating
    components in the critical path.</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34715"
  slides: ''
  video: "//dl.acm.org/authorize?N34715"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: 'VR is on the edge: how to deliver 360° videos in mobile networks'
  authors: Simone Mangiante and Guenter Klas (Vodafone Group R&D), Amit Navon, Zhuang
    GuanHua, and Ju Ran (Huawei), and Marco Dias Silva (Vodafone Group R&D)
  abstract: "<p>VR/AR is rapidly progressing towards enterprise and end customers
    with the promise of bringing immersive experience to numerous applications. Soon
    it will target smartphones from the cloud and 360° video delivery will need unprecedented
    requirements for ultra-low latency and ultra-high throughput to mobile networks.
    Latest developments in NFV and Mobile Edge Computing reveal already the potential
    to enable VR streaming in cellular networks and to pave the way towards 5G and
    next stages in VR technology. In this paper we present a Field Of View (FOV) rendering
    solution at the edge of a mobile network, designed to optimize the bandwidth and
    latency required by VR 360° video streaming. Preliminary test results show the
    immediate benefits in bandwidth saving this approach can provide and generate
    new directions for VR/AR network research.</p>\n"
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34716"
  slides: ''
  video: "//dl.acm.org/authorize?N34716"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: 'VR/AR Immersive Communication: Caching, Edge Computing, and Transmission
    Trade-Offs'
  authors: Jacob Chakareski (The University of Alabama)
  abstract: '<p>We study the delivery of <span class="math inline">360<sup>∘</sup></span>-navigable
    videos to 5G VR/AR wireless clients in future cooperative multi-cellular systems.
    A collection of small-cell base stations interconnected via back-haul links are
    sharing their caching and computing resources to maximize the aggregate reward
    they earn by serving <span class="math inline">360<sup>∘</sup></span> videos requested
    by VR/AR wireless clients. We design an efficient representation method to construct
    the <span class="math inline">360<sup>∘</sup></span> videos such that they only
    deliver the remote scene viewpoint content genuinely needed by the VR/AR user,
    thereby overcoming the present highly inefficient approach of sending a bulky
    <span class="math inline">360<sup>∘</sup></span> video, whose major part comprises
    scene information never accessed by the user. Moreover, we design an optimization
    framework that allows the base stations to select cooperative caching/rendering/streaming
    strategies that maximize the aggregate reward they earn when serving the users,
    for the given caching/computational resources at each base station. We formulate
    the problem of interest as integer programming, show its NP-hardness, and derive
    a fully-polynomial-time approximation solution with strong performance guarantees.
    Our advances demonstrate orders of magnitude operational efficiency gains over
    state-of-the-art caching and <span class="math inline">360<sup>∘</sup></span>
    video representation mechanisms and are very promising. This is a first-of-its-kind
    study to explore fundamental trade-offs between caching, computing, and communication
    for emerging VR/AR applications of prospectively broad societal impact.</p>

'
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34717"
  slides: ''
  video: "//dl.acm.org/authorize?N34717"
  COL_UID: ''
- type: paper
  time: ''
  room: ''
  title: Delivering deep learning to mobile devices via offloading
  authors: Xukan Ran and Haoliang Chen (University of California, Riverside), Zhenming
    Liu (College of William and Mary), and Jiasi Chen (University of California, Riverside)
  abstract: |
    <p>Deep learning could make Augmented Reality (AR) devices smarter, but few AR apps use such technology today because it is compute-intensive while front-end devices often cannot deliver sufficient compute power. We propose a distributed framework that ties together front-end devices with more powerful backend “helpers” that allow deep learning to be executed locally or to be offloaded. The framework shall be able to intelligently use current estimates of network conditions and backend server loads, and in conjunction with application’s requirements to determine an optimal offload strategy.</p>
    <p>This work reports our preliminary investigation in implementing this framework, in which the front-end is assumed to be smartphones. Our specific contributions include (1) development of an Android application that performs real-time object detection, either locally on the smartphone or remotely on a server; and (2) characterization of the tradeoffs between object detection accuracy, latency, and battery drain, based on the system parameters of video resolution, CNN model size, and offloading decision.</p>
  bio: ''
  photo: ''
  link: "//dl.acm.org/authorize?N34718"
  slides: ''
  video: "//dl.acm.org/authorize?N34718"
  COL_UID: ''
- type: break
  time: 3:30pm - 4:00pm
  room: Foyer
  title: Coffee Break
  authors: ''
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
- type: session
  time: 4:00pm - 5:30pm
  room: Illumination Room (Luskin Center)
  title: Open Discussion and closing remarks
  authors: Yong Liu
  abstract: ''
  bio: ''
  photo: ''
  link: ''
  slides: ''
  video: ''
  COL_UID: ''
